{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Duch W, Adamczak R, Grabczewski K (1996) Extraction of logical rules from training data using backpropagation networks, in: Proc. of the The 1st Online Workshop on Soft Computing, 19-30.Aug.1996, pp. 25-30. (Web Link)\n",
    "Duch, Adamczak, and Grabczewski develop a method for extracting a rule set for determining categorization by developing multi-layered perceptrons. In order to express logical rules, they first determine a method for mapping real values into discrete categories which still allows categorization of all vectors. Using these new inputs and a neural network, they use backpropagation with a steep sigmoid function for activation, and an error function that better enforces values of +1, -1, and 0. These weights determine whether a feature is significant for categorization, insignificant, or if the opposite feature is significant respectively. These weights are then analyzed in order to determine the corresponding ruleset. This method, used on the Iris dataset, produces a ruleset that is able to classify 98% of the data using at most three rules for one category. This method was also used on the mushroom dataset, but they were unable to determine the corresponding ruleset due to the complex nature of the dataset.\n",
    "http://www.fizyka.umk.pl/ftp/pub/papers/kmk/96lrules.pdf\n",
    "\n",
    "Robert Andrews, Joachim Diederich, Alan B. Tickle, Survey and critique of techniques for extracting rules from trained artificial neural networks, Knowledge-Based Systems, Volume 8, Issue 6, December 1995, Pages 373-389, ISSN 0950-7051. (Web Link)\n",
    "Andrews, Diederich, and Tickle survey & critique techniques for extracting rules from trained artificial neural networks. They focus on mechanisms, procedures, and algorithms for knowledge initialization, rule extraction, and rule refinement. There is some discussion as to why it is beneficial to have artificial neural networks (ANNs) have techniques for rule extraction. They argue that knowledge acquisition is less complicated for neural networks than for rules based systems, that there is a greater speed to access data and it is simpler to store. They also argue that ANNs are robust in the face of noise and accurate in their categorization. The major drawback they wish to address (and part of the need for rule extraction) is that the neural network can’t explain why it got the answer it got. Adding rule extraction mechanisms improves the explanatory power, the generalization of ANN solutions, and it can overcome the knowledge acquisition problem for symbolic AI systems.\n",
    "They review rule extraction methods from multilayered ANNs that use supervised learning regimes (such as backpropagation). The problems/questions they attempt to address are various methods on the basis of knowledge acquired in the training phase encoded in the architecture (the number of hidden units), an activation function, and set of real-valued numerical parameters. They classify the expressive power, translucency, extent of underlying training regimes, quality of extracted rules, and complexity of extraction & refinement. They group approaches into two main categories: decompositional approaches and pedagogical approaches. In conclusion, they find that being able to extract “fuzzy” rules improves the explanatory power of neural networks. \n",
    "\n",
    "Duch, Wlodzislaw, Rafal Adamczak, and Krzysztof Grøbczewski. \"A new methodology of extraction, optimization and application of crisp and fuzzy logical rules.\" Neural Networks, IEEE Transactions on 12.2 (2001): 277-306. (Web Link)\n",
    "Duch, Adamczak, and Grøbczewski expand on their 1996 paper in order to fully apply their method of logical extraction from multi-layered perceptrons on the mushroom dataset, as well as multiple other datasets. They also compare results across these datasets between different methods of rule extraction. Their result proved to be able to produce simple sets of rules that correctly predicted most of the data. For data that they couldn’t predict as well, their rule system they produced exposed underlying problems with the data itself. Finally, they were able to demonstrate that for two specific datasets, the network and ruleset they developed were more accurate than more general neural networks. This could be due to the inability of softer transfer functions to represent sharp decision boundaries that might be necessary for some feature sets. Additionally, their method uses additional global optimization methods that might find a better optimal solutions than the gradient-descent method that neural networks use.\n",
    "http://www.fizyka.umk.pl/publications/kmk/00-tnn.pdf\n",
    "\n",
    "Lichman, M. (2013). Mushroom Data Set. UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROJECT START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/20/16-5/1/16: Reviewed literature and settled on the MLP2LN for the Iris and Mushroom datasets. Proposal written, submitted, accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/5/16: Reviewed timeline and set sub-deadlines.\n",
    "6th sunday: write algorithm\n",
    "7th week: make model!\n",
    "8th Wednesday 5/18: Running model\n",
    "9th Monday 5/23: Have running model on data, data analysis, outline of presentation/paper\n",
    "9th Friday: presentation\n",
    "10th wednesday: paper due\n",
    "\n",
    "Action Steps:\n",
    "1. Review papers, attempt to glean basic idea / algorithm.\n",
    "2. Put together algorithm.\n",
    "3. Write basic class structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/8/16: Reviewed papers on MLP2LN algorithms to start writing pseudocode. \n",
    "Important equations:\n",
    "\n",
    "For labeling features (eq 2): IF $x_i \\in  X_{ij}$ THEN $(s_k=label(x_k) = T)$\n",
    "\n",
    "Weight pruning (eq 4): $W_{ij} \\leftarrow W_{ij} + \\lambda W_{ij} (W_{ij}^2 -1) (3W_{ij}^2 - 1)$\n",
    "\n",
    "Weight training (eq 9 in p1, 10 in p2): \n",
    "$\\frac{1}{2}\\Sigma_p\\Sigma_k (Y_k^p - A_W(X^p)_k)^2 + \\frac{\\lambda_1}{2} \\Sigma_{i>j} W^2_{ij} + \\frac{\\lambda_2}{2}\\Sigma_{i>j}W^2_{ij} (W_{ij} - 1)^2 (W_{ij} + 1)^2$\n",
    "\n",
    "\n",
    "Algorithm (adapted from page 10 of 2nd paper):\n",
    "1. Create one hidden neuron\n",
    "2. Train that neuron on data using backpropogation with regularization. $\\lambda_1 = 10^{-5}$ and $\\lambda_2 = 0$\n",
    "3. Train as long as the error decreases. Then increase $\\lambda_1$ by a factor of $10$ until a sharp increase of the error. If after increasing $\\lambda_1$ there's an increase of a factor of 5 in the error, stop. Decrease $\\lambda_1$ returns to previous state. Remove weights smaller than $|.1|$. Set $\\lambda_2 = \\lambda_1$ and $\\lambda_1 = 0$. Train slowly, increasing the slopes in $\\lambda_2$ until weights reach $0 \\pm 0.05$ or $\\pm 1 \\pm 0.05$. Set very large T (about one thousand) and set integer weights to $0 , \\pm 1$. \n",
    "4. Analyse the weights and the thresholds obtained \n",
    "5. Freeze the weights of existing neurons. \n",
    "6. Add the next neurone. Be sure to connect it to the output neuron.\n",
    "7. Repeat the procdedure until all data is correctly classified or the number of rules obtained grows sharply.\n",
    "\n",
    "\n",
    "Questions:\n",
    "1. What's the unit slope / sigmoidal function (step 2, step 3a)?\n",
    "2. How do they get the rules out of the network?\n",
    "3. Why doesn't Fig 1 match the table for the Iris problem?\n",
    "4. What are the deltas?\n",
    "\n",
    "\n",
    "Next Steps:\n",
    "1. Figure out how to get the rules from the network\n",
    "2. How do we get the threshold $\\theta$ and the $\\delta$\n",
    "3. Go ask Anna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5/11/16: Anna's Office Hours Notes\n",
    "\n",
    "1. Not sure - email papers & questions to her. \n",
    "Rephrased questions:\n",
    "1. We're not sure what T is. It shows up in eq 2 of the short paper and seems to represent a numerical label of the data. In the longer paper (on page 10), T seems to be the unit slope, or the unit slope is defined by a sigma of x/T. We don't really know what the unit slope is. Or what T is. Or what x is. \n",
    "2. We understand how the arrive at the weights given on page 2 of the shorter paper, but not how they reach the \"threshold value\" or theta. We understand how the network is built & how it's trained, with the exception of the Ts above. We're confused because the weights they give don't seem to match Fig 1 that's supposed to represent the structure of the network. On page three, they show the rules they've extracted from the network, but we don't know how they get from the weights & network structure to those rules. \n",
    "3. In Table 1 we're not sure how they're getting the delta values. The values don't seem consistent with the explanation they give for how they calculate the deltas above it. \n",
    "4. We *think* that there's some sort of mapping from the network to the tree in Fig 2 of the shorter paper. And from there, you extract the rules. It's not clear to us how that is happening. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/11/16: Anna's E-Mail\n",
    "1. So, big picture I think they're trying to ensure that the activations of the units are just 0/1 (or -1/1). They're using a sigmoid as g: 1/(1+exp(-x)). You can offset this to have asymptotes at -1/1 rather than 0/1 if you modify as follows: g(x) = -1+2/(1+exp(-x)) (see wikipedia https://en.wikipedia.org/wiki/Generalised_logistic_function for more on variations on this). I think they're saying this sigmoid has unit slope. You can increase the slope to make it sharper, so that the transition from 0 to 1 happens faster, if you multiply x by something. E.g., in the graph below, the red line is g(x) = 1/(1+exp(-5x)). \n",
    "I suspect that T is meant to increase the slope so that the hidden units will all have activations near -1/1, meaning that we have a \"logical\" form of the real valued inputs. (x here is the weights * inputs).\n",
    "\n",
    "Ways this is inconsistent/confusing from the papers:\n",
    "- They sound like they're dividing by T rather than multiplying. I think this doesn't make sense because I think they want the slope to get steeper and steeper slopes based on \"gradually increasing the slope β of sigmoidal functions σ(βx) to obtain crisp decision regions\"\n",
    "- I have no idea what T is in the shorter paper, although I'm sort of suspecting it's not the same as this T in the loner paper.\n",
    "2 & 3. I'm not entirely sure what the threshold values mean, although page 111 of the longer paper maybe has an answer in the paragraph right before \"C. probability density networks\"?\n",
    "\n",
    "I understand part of the bit about the rules and the deltas, although not entirely. This figure makes the most sense to me on relating the structure of the network (I agree, fig 1 doesn't make sense to me):\n",
    "Inline image 2\n",
    "Here, for all three types of irises, the value for input 1 doesn't matter, so none of the s/m/l associate with $X_{1}$ have outgoing arrows. Similarly for $X_{2}$. For $X_{3}$, there's one for each one of the three class (three outgoing arrows)  and similarly for $X_{4}$.\n",
    "\n",
    "\n",
    "Going back to the other paper, I think the way they're going from the weights to the rules is as follows:\n",
    "Inline image 3\n",
    "Inline image 5\n",
    "First, the tree is only going to tell us where this is Setosa or not. They create the table above by looking at each section of the weight vector for setosa. The weights for x_{1} are (+, 0, 0), meaning that if x_{1} is small, it gets a positive weight (and they've constrained their weights to be very near +1 or -1), so the contribution to the final output is approximately 1. If it's not small, then that feature is equal to -1, so with the positive weight, the contribution to the final output is approximately -1. Same thing for $x_{2}$. If $x_{3}$ is small, then it has a positive contribution from the small weight, and it also has medium = -1, so it gets a positive contribution from -1*(negative medium weight). Thus, total contribution to the output is +2. There's a similar argument for if $x_{3}$ is medium, and if $x_{3}$ is large, then it has both small = medium = -1, so these weights cancel each other out.\n",
    "Inline image 4\n",
    "Once we have the table above, we can make this tree. They chose an ordering on the input features, and are basically looking at whether with these input features, the output could ever fall below the threshold of 2. (I'm not sure why they're putting 3 - it may end up being the same thing, but I think it would be less confusing if they used 2...). Once we look at $x_{4}$, we're either +3 or -1 depending on whether $x_{4}$ was small. Then, we look at $x_{3}$. If x4 = not small and x3 = large, then total contribution to the output is -1. If we look at $x_{2}$ and $x_{1}$, the most they could swing us to is 1. This is less than the threshold for activation (theta = 2), so no matter what, we won't say Setosa. We're basically cutting off the tree based on whether looking at further inputs could actually influence our classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May 16, 2016.\n",
    "\n",
    "Agenda:\n",
    "1. Write out algorithm for retrieving deltas.\n",
    "2. Start writing code? Method headings, main method. \n",
    "3. How are we going to make/search the tree?\n",
    "\n",
    "Notes:\n",
    "1. Each edge between a feature and the hidden layer has a weight in a network is 0+/-1. To get the delta: \n",
    "for each identifier (small/medium/large):\n",
    "    if #it's activated, then add the weight to the delta\n",
    "    if #it's not activated, then subtract the weight from the delta\n",
    "    if #its' zero, do nothing\n",
    "\n",
    "Given the Deltas for each idenitifier for each category, we first need to order categories such that categories that have the most effect on activation (the absolute value of the sum of the deltas) will be the first levels of the tree. After creating this ordering, we need to calculate the \"worst case\" for each level (the sum of the smallest delta for each category below in the tree). We will also calculate the \"best case\" for each level. With this we can create a tree in order to find the different possible activations (represented by each node). If at a certain node we find that the activatino is high enough so that, even in the worst case, we know it will pass the threshold, we can stop expanding the tree at that point and make a rule. If an activation is low enough so that the best case will not bring it up to the threshold, we will stop expanding the tree at that point (and not make a rule).\n",
    "\n",
    "2. We wrote skeleton code\n",
    "3. See above. \n",
    "\n",
    "Next Steps:\n",
    "1. fill out code\n",
    "2. investigate thresholds & Ts\n",
    "3. see if we can steal Anna's backprop code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/17/16:\n",
    "We started filling out the code, verified that we could steal Anna's backprop code. load_data, labeling, and training happens below! The network currently never classifies anything as Iris-versicolor. This seems weird. We're going to sleep on it and try again tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with first stage\n"
     ]
    }
   ],
   "source": [
    "import mushroom\n",
    "import numpy as np\n",
    "import multilayernetwork as mn\n",
    "data = mushroom.load_data('data/iris.data')\n",
    "X, expected_outputs = mushroom.label_features(data)\n",
    "#print(X)\n",
    "X = np.array(X).T\n",
    "Y = np.array(expected_outputs).T\n",
    "\n",
    "weights, bias = mn.train_multilayer_network(X, Y, mn.update_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ -1.36493453e+00,   1.14190321e+00,   8.45169159e-01,\n",
      "         -1.20903302e+00,  -4.58595845e-01,   1.57949634e+00,\n",
      "         -3.02658879e+00,  -4.20366625e-01,   2.05147549e+00,\n",
      "         -1.26271838e+00,  -8.25757272e-01,   6.35690402e-01],\n",
      "       [  1.54487748e+00,  -8.29619635e-01,  -5.65562888e-01,\n",
      "         -4.13638660e-01,   2.25850604e+00,   2.31876165e+00,\n",
      "          1.13700628e+00,  -1.40085536e-01,   1.09661662e+00,\n",
      "          1.68730808e+00,  -1.53437827e+00,   1.49184135e+00],\n",
      "       [ -6.77401962e-01,  -2.97437133e-01,   3.53600851e-01,\n",
      "         -1.75847073e+00,  -1.38811571e-01,  -3.58966617e-01,\n",
      "          4.54607233e-01,  -2.12484292e+00,  -3.29438104e+00,\n",
      "         -4.77286807e-02,   1.93846789e+00,  -1.33774042e+00],\n",
      "       [  7.34747208e-01,   5.73992131e-01,  -1.38099921e+00,\n",
      "         -4.96800274e-02,  -9.33928943e-01,  -1.41901970e+00,\n",
      "          3.57454863e-01,   6.96124287e-01,  -3.23062391e-01,\n",
      "          7.86080079e-01,   2.90821931e+00,  -3.94817438e-01],\n",
      "       [  1.27830309e+00,   2.48358724e+00,  -1.81100509e+00,\n",
      "         -1.30413344e+00,   1.77144718e+00,  -7.95905128e-01,\n",
      "          2.36523104e-01,   1.14028324e+00,   6.49737852e-01,\n",
      "          2.18726269e+00,   1.67963672e+00,   7.94492657e-01],\n",
      "       [ -7.24271355e-01,   9.48073650e-01,   3.32187244e-01,\n",
      "         -1.50801418e+00,   1.66700826e+00,   1.03228979e+00,\n",
      "          2.31511501e-01,  -2.18261101e-01,  -1.40475086e+00,\n",
      "          4.05682433e-01,   5.85860890e-01,  -1.00048680e+00],\n",
      "       [  9.93711433e-01,  -1.27341646e+00,  -1.90942847e+00,\n",
      "          1.29521509e+00,  -3.58854213e-01,  -2.21291333e-01,\n",
      "         -1.28449308e+00,  -1.88813970e+00,   1.96707339e+00,\n",
      "         -1.11641255e+00,   1.76423676e+00,   1.50221043e+00],\n",
      "       [ -2.89937277e-01,  -1.29162189e+00,  -7.12896544e-01,\n",
      "         -1.12272147e+00,   2.30451670e+00,   1.10600312e-01,\n",
      "          1.41489449e+00,  -1.64340277e+00,   4.18280081e-01,\n",
      "          1.44616642e+00,  -7.60982640e-01,   1.55129729e+00],\n",
      "       [ -1.79710281e+00,  -1.16361369e+00,   6.23494347e-01,\n",
      "         -3.59518997e-01,   4.40522127e-01,  -2.01836600e+00,\n",
      "         -5.31732413e-02,   3.63163734e+00,   1.38902240e-01,\n",
      "         -8.19947037e-01,   8.35458333e-01,  -1.87742552e+00],\n",
      "       [  5.25740640e-01,  -1.69259575e+00,  -6.50097303e-01,\n",
      "          9.18827879e-01,  -3.96409298e-01,   6.18443908e-01,\n",
      "          1.22063355e+00,   5.35947301e-01,  -1.84637167e+00,\n",
      "         -3.00441310e-01,   2.41932607e+00,  -2.56897282e+00],\n",
      "       [ -1.18639198e+00,  -1.44226516e-01,   5.84179565e-01,\n",
      "          1.17488047e+00,   1.45460016e+00,  -2.16861061e-01,\n",
      "         -3.86777899e-01,   1.59340958e+00,   8.66491237e-01,\n",
      "         -1.94296300e+00,  -8.16514961e-02,  -9.31432147e-01],\n",
      "       [ -6.48574998e-01,  -3.02461511e-04,   9.90957941e-01,\n",
      "          3.62954971e-01,  -5.22838370e-01,   1.92022229e+00,\n",
      "          2.23277962e+00,  -1.40035462e+00,  -4.73117451e-01,\n",
      "          6.62589966e-01,   3.73928188e-01,   8.93198191e-01]]), array([[-2.45465892, -0.13751666,  1.25651813,  1.26097454,  0.3621196 ,\n",
      "         0.42240493, -1.00088281,  1.54418558, -2.80812905,  0.02055753,\n",
      "        -2.92373888,  1.02264833],\n",
      "       [-0.53848696, -1.97477065,  1.85693428,  1.5043188 , -0.23745435,\n",
      "         0.43815056, -0.60204794, -1.76911595,  2.48423915,  0.8275242 ,\n",
      "         1.56570225, -2.12938294],\n",
      "       [ 2.28771337, -0.20358067, -2.08719579, -0.61517998, -1.50559882,\n",
      "        -0.56001748,  1.63540297,  1.40896747,  0.17789378, -2.96575623,\n",
      "         0.41602091, -0.21778969]]))\n",
      "[[ 50.   0.   0.]\n",
      " [  0.  48.   2.]\n",
      " [  0.   3.  47.]]\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "\n",
    "Y = mn.predict_multilayer_network(X, weights, bias, mn.logistic, mn.logistic, 1)\n",
    "output = Y.T\n",
    "for i in range(len(output)):\n",
    "    j = np.argmax(output[i])\n",
    "    output[i] = [0,0,0]\n",
    "    output[i][j] = 1\n",
    "print(mn.get_confusion_matrix(output, expected_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5/23/16: We fixed the problem that stopped the network from classifying Iris-Versicolor things by increasing the number of hidden units. We started writing our own error functions as specified in the paper, but we're stuck because:\n",
    "* we're not sure how to add in the \"additional weight updates\" to the \"cost function\" because we're not sure what the cost function is\n",
    "* we're alo not totally sure if we should be using cross entropy or not, how that works, and if the one we have is broken \n",
    "* by changing the g for the second layer to multinomial (which might be what the cross entropy update is?) we mess up our predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5/25/16:\n",
    "We stuck T in a bunch of places, fixed our error functions, started implementing the proper training algorithm. Got as far as stopping when the lambda is no longer reasonable. We crashed our kernal 5 times until we learned to interrupt it. We're still confused about Anna's dlogistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
